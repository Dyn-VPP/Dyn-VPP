<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Dyn-VPP: Video Prediction Policy Optimization for Improved Visual Dynamics</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="description" content="Dyn-VPP: Video Prediction Policy Optimization for Improved Visual Dynamics in Visual-Language-Action Models across CALVIN, L-CALVIN, and real-world benchmarks." />
  <style>
    :root {
      --bg: #0b0c10;
      --bg-alt: #12141c;
      --fg: #e5e7eb;
      --accent: #fbbf24;
      --accent-soft: rgba(251, 191, 36, 0.15);
      --muted: #9ca3af;
      --border: #1f2937;
      --link: #38bdf8;
      --code-bg: #111827;
    }
    * { box-sizing: border-box; margin: 0; padding: 0; }
    body {
      font-family: system-ui, -apple-system, BlinkMacSystemFont, "SF Pro Text",
        "Segoe UI", sans-serif;
      background: radial-gradient(circle at top left, #111827 0, #020617 50%, #000 100%);
      color: var(--fg);
      line-height: 1.6;
    }
    a { color: var(--link); text-decoration: none; }
    a:hover { text-decoration: underline; }
    header {
      position: sticky;
      top: 0;
      z-index: 20;
      backdrop-filter: blur(14px);
      background: linear-gradient(to bottom, rgba(15, 23, 42, 0.96), rgba(15, 23, 42, 0.8), transparent);
      border-bottom: 1px solid rgba(55, 65, 81, 0.7);
    }
    .nav {
      max-width: 1040px;
      margin: 0 auto;
      padding: 0.75rem 1.25rem;
      display: flex;
      align-items: center;
      justify-content: space-between;
      font-size: 0.95rem;
    }
    .nav-left {
      display: flex;
      align-items: center;
      gap: 0.5rem;
      font-weight: 600;
      letter-spacing: 0.03em;
      text-transform: uppercase;
      color: var(--muted);
    }
    .badge {
      font-size: 0.7rem;
      padding: 0.2rem 0.5rem;
      border-radius: 999px;
      border: 1px solid rgba(148, 163, 184, 0.8);
      text-transform: uppercase;
      letter-spacing: 0.12em;
      color: #e5e7eb;
    }
    .nav-links {
      display: flex;
      align-items: center;
      gap: 1rem;
      font-size: 0.9rem;
    }
    .nav-links a {
      color: var(--muted);
      text-decoration: none;
    }
    .nav-links a:hover {
      color: var(--fg);
    }

    main {
      max-width: 1040px;
      margin: 0 auto;
      padding: 2.5rem 1.25rem 4rem;
    }

    .hero {
      display: grid;
      grid-template-columns: minmax(0, 2.2fr) minmax(0, 1.8fr);
      gap: 2.5rem;
      align-items: center;
      padding-bottom: 2.5rem;
      border-bottom: 1px solid var(--border);
    }
    @media (max-width: 900px) {
      .hero {
        grid-template-columns: minmax(0, 1fr);
      }
    }
    .hero-title {
      font-size: clamp(2.1rem, 3vw, 2.6rem);
      font-weight: 700;
      letter-spacing: 0.01em;
      margin-bottom: 1rem;
    }
    .hero-subtitle {
      font-size: 1.05rem;
      color: var(--muted);
      max-width: 34rem;
      margin-bottom: 1.25rem;
    }
    .hero-meta {
      display: flex;
      flex-wrap: wrap;
      gap: 0.5rem;
      margin-bottom: 1.25rem;
      font-size: 0.85rem;
      color: var(--muted);
    }
    .hero-actions {
      display: flex;
      flex-wrap: wrap;
      gap: 0.75rem;
      margin-bottom: 0.75rem;
    }
    .btn {
      display: inline-flex;
      align-items: center;
      justify-content: center;
      gap: 0.4rem;
      padding: 0.55rem 0.95rem;
      border-radius: 999px;
      font-size: 0.9rem;
      font-weight: 500;
      border: 1px solid rgba(148, 163, 184, 0.7);
      background: radial-gradient(circle at top left, rgba(17, 24, 39, 0.9), rgba(15, 23, 42, 0.9));
      color: var(--fg);
    }
    .btn-primary {
      border-color: rgba(252, 211, 77, 0.9);
      background: radial-gradient(circle at top left, rgba(251, 191, 36, 0.18), rgba(24, 24, 27, 0.96));
      box-shadow: 0 0 40px rgba(250, 204, 21, 0.15);
    }
    .btn:hover {
      text-decoration: none;
      border-color: rgba(248, 250, 252, 0.9);
    }
    .hero-note {
      font-size: 0.8rem;
      color: var(--muted);
    }

    .hero-card {
      background: radial-gradient(circle at top left, rgba(251, 191, 36, 0.06), rgba(15, 23, 42, 0.98));
      border-radius: 1.1rem;
      padding: 1rem;
      border: 1px solid rgba(148, 163, 184, 0.6);
      box-shadow: 0 22px 55px rgba(0, 0, 0, 0.7);
    }
    .hero-card-inner {
      border-radius: 0.7rem;
      padding: 1rem;
      background: radial-gradient(circle at top right, rgba(56, 189, 248, 0.22), rgba(15, 23, 42, 0.96));
      border: 1px solid rgba(55, 65, 81, 0.9);
    }
    .kpi-row {
      display: flex;
      flex-wrap: wrap;
      gap: 1rem;
      margin-bottom: 0.75rem;
    }
    .kpi {
      flex: 1 1 120px;
      padding: 0.45rem 0.65rem;
      border-radius: 0.7rem;
      background: rgba(15, 23, 42, 0.9);
      border: 1px solid rgba(55, 65, 81, 0.9);
      font-size: 0.8rem;
    }
    .kpi-label {
      color: var(--muted);
      font-size: 0.75rem;
      margin-bottom: 0.15rem;
      text-transform: uppercase;
      letter-spacing: 0.12em;
    }
    .kpi-value {
      font-weight: 600;
      font-size: 0.98rem;
    }
    .hero-card-caption {
      font-size: 0.8rem;
      color: var(--muted);
      margin-top: 0.4rem;
    }

    section {
      padding-top: 2.5rem;
      padding-bottom: 1.25rem;
      border-bottom: 1px solid var(--border);
    }
    section:last-of-type {
      border-bottom: none;
    }
    .section-title {
      font-size: 1.25rem;
      font-weight: 600;
      margin-bottom: 0.75rem;
    }
    .section-subtitle {
      font-size: 0.95rem;
      color: var(--muted);
      margin-bottom: 1rem;
    }

    .grid-2 {
      display: grid;
      grid-template-columns: minmax(0, 1fr) minmax(0, 1fr);
      gap: 1.75rem;
    }
    @media (max-width: 900px) {
      .grid-2 {
        grid-template-columns: minmax(0, 1fr);
      }
    }
    h3 {
      font-size: 1.05rem;
      margin: 0.6rem 0 0.3rem;
    }
    p { margin-bottom: 0.6rem; }

    ul {
      margin-left: 1.1rem;
      margin-bottom: 0.6rem;
    }
    li { margin-bottom: 0.3rem; }

    .pill {
      display: inline-flex;
      align-items: center;
      gap: 0.35rem;
      padding: 0.15rem 0.5rem;
      border-radius: 999px;
      border: 1px solid rgba(148, 163, 184, 0.6);
      font-size: 0.75rem;
      color: var(--muted);
      margin-right: 0.25rem;
      margin-bottom: 0.25rem;
    }

    .tag-row {
      display: flex;
      flex-wrap: wrap;
      gap: 0.3rem;
      margin-bottom: 0.4rem;
    }

    .card {
      border-radius: 0.9rem;
      padding: 0.9rem 1rem;
      background: rgba(15, 23, 42, 0.9);
      border: 1px solid rgba(55, 65, 81, 0.9);
      margin-bottom: 0.7rem;
      font-size: 0.9rem;
    }
    .card-title {
      font-weight: 600;
      margin-bottom: 0.25rem;
    }
    .card-metric {
      font-size: 0.9rem;
      color: var(--muted);
      margin-bottom: 0.3rem;
    }

    pre {
      background: var(--code-bg);
      border-radius: 0.6rem;
      padding: 0.75rem 0.9rem;
      font-size: 0.8rem;
      overflow-x: auto;
      border: 1px solid rgba(31, 41, 55, 0.9);
    }
    code {
      font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono",
        "Courier New", monospace;
    }

    footer {
      padding-top: 2rem;
      font-size: 0.8rem;
      color: var(--muted);
      text-align: center;
    }
  </style>
</head>
<body>
  <header>
    <div class="nav">
      <div class="nav-left">
        <span>Dyn-VPP</span>
        <span class="badge">VPM Post-Training</span>
      </div>
      <nav class="nav-links">
        <a href="#overview">Overview</a>
        <a href="#method">Method</a>
        <a href="#results">Results</a>
        <a href="#bibtex">BibTeX</a>
      </nav>
    </div>
  </header>

  <main>
    <section class="hero">
      <div>
        <h1 class="hero-title">
          Dyn-VPP: Video Prediction Policy Optimization for Improved Visual Dynamics
        </h1>
        <p class="hero-subtitle">
          A post-training framework that directly optimizes the visual dynamics of video prediction models,
          translating better dynamics into stronger long-horizon manipulation across
          <strong>CALVIN ABCâ†’D</strong>, <strong>L-CALVIN</strong>, and <strong>real-world</strong> benchmarks.
        </p>
        <div class="hero-meta">
          <span>Wenxuan Song</span><span>Â·</span>
          <span>Han Zhao</span><span>Â·</span>
          <span>Fuhao Li</span><span>Â·</span>
          <span>Ziyang Zhou</span><span>Â·</span>
          <span>Pengxiang Ding</span><span>Â·</span>
          <span>Donglin Wang</span><span>Â·</span>
          <span>Haoang Li</span>
        </div>
        <div class="hero-meta">
          <span>HKUST (GZ)</span><span>Â·</span>
          <span>Zhejiang University</span><span>Â·</span>
          <span>Westlake University</span><span>Â·</span>
          <span>Tsinghua University</span><span>Â·</span>
          <span>Shandong University</span><span>Â·</span>
          <span>OpenHelix Team</span>
        </div>
        <div class="hero-actions">
          <!-- TODO: replace hrefs with real links -->
          <a class="btn btn-primary" href="#" target="_blank" rel="noreferrer">
            ðŸ“„ Paper (arXiv)
          </a>
          <a class="btn" href="#" target="_blank" rel="noreferrer">
            ðŸ’» Code (GitHub)
          </a>
          <a class="btn" href="#" target="_blank" rel="noreferrer">
            ðŸŽ¥ Video
          </a>
        </div>
        <p class="hero-note">
          Contact: <a href="mailto:dynvpp@126.com">dynvpp@126.com</a>
        </p>
      </div>

      <div class="hero-card">
        <div class="hero-card-inner">
          <div class="kpi-row">
            <div class="kpi">
              <div class="kpi-label">CALVIN ABCâ†’D</div>
              <div class="kpi-value">+0.28 â†‘ Avg. Len</div>
              <div class="card-metric">98.0 â†’ 94.8 â†’ 91.3 â†’ 88.3 â†’ 83.1</div>
            </div>
            <div class="kpi">
              <div class="kpi-label">L-CALVIN</div>
              <div class="kpi-value">+1.2 â†‘ Avg. Len</div>
              <div class="card-metric">5.53 â†’ <strong>6.73</strong></div>
            </div>
          </div>
          <div class="kpi-row">
            <div class="kpi">
              <div class="kpi-label">Visionâ€“Action Coupling</div>
              <div class="kpi-value">ER / ERR â†‘</div>
              <div class="card-metric">richer Jacobian spectrum vs. base policy</div>
            </div>
            <div class="kpi">
              <div class="kpi-label">Real World</div>
              <div class="kpi-value">Consistent Gains</div>
              <div class="card-metric">Agibot Genie 01, multi-task benchmarks</div>
            </div>
          </div>
          <p class="hero-card-caption">
            Dyn-VPP post-trains a pretrained VPM with GRPO in latent space, improving visual dynamics and
            yielding stronger long-horizon policies across multiple simulated and real-world benchmarks.
          </p>
        </div>
      </div>
    </section>

    <section id="overview">
      <h2 class="section-title">1. Overview</h2>
      <p class="section-subtitle">
        Inspired by video-action models such as
        <a href="https://mimic-video.github.io/" target="_blank" rel="noreferrer">mimic-video</a>
        and efficient VLA paradigms like
        <a href="https://vla-adapter.github.io/" target="_blank" rel="noreferrer">VLA-Adapter</a>,
        Dyn-VPP focuses on directly improving <em>visual dynamics</em> inside a pretrained video prediction
        backbone, rather than redesigning the full VLA stack.
      </p>
      <div class="grid-2">
        <div>
          <h3>Motivation</h3>
          <p>
            Modern VLAs excel at semantic grounding but often fall short on precise physical dynamics,
            especially for long-horizon, language-conditioned manipulation.
            Existing VPM-based VLAs learn dynamics via supervised training on trajectories,
            which can leave residual mismatch between predicted and expert dynamics.
          </p>
          <p>
            Dyn-VPP treats the VPMâ€™s denoising trajectory as a sequence of actions and
            <strong>optimizes it directly with reinforcement learning</strong>, using a reward that compares
            predicted futures to expert trajectories in latent space. This upgrades the dynamics
            quality without altering the architecture of the base policy.
          </p>
        </div>
        <div>
          <h3>Contributions</h3>
          <ul>
            <li>
              <strong>Visual-dynamics-centric post-training</strong>: a GRPO-based optimization strategy
              that directly updates VPM denoising in latent space.
            </li>
            <li>
              <strong>Multi-benchmark validation</strong>:
              consistent improvements on <strong>CALVIN ABCâ†’D</strong>,
              <strong>L-CALVIN</strong>, and multiple <strong>real-world tasks</strong> on Agibot and beyond.
            </li>
            <li>
              <strong>Mechanistic analysis</strong> of visionâ€“action coupling via
              effective rank of the Jacobian, showing richer coupling after post-training.
            </li>
            <li>
              <strong>Plug-in design</strong>: no change to the backbone architecture;
              Dyn-VPP seamlessly boosts existing VPM-based VLAs.
            </li>
          </ul>
        </div>
      </div>
    </section>

    <section id="method">
      <h2 class="section-title">2. Method: Dyn-VPP in a Nutshell</h2>
      <p class="section-subtitle">
        Dyn-VPP treats a pretrained video prediction model as a policy over denoising steps, and
        applies group-wise PPO (GRPO) to align its predictions with expert dynamics in latent space.
      </p>

      <div class="grid-2">
        <div>
          <h3>2.1 Visual Dynamics Objective</h3>
          <p>
            Let the VPM map an initial noisy latent to a sequence of intermediate latents
            \( \{z_t\} \) along a denoising trajectory. Dyn-VPP defines a reward that measures
            how well the generated future matches an expert future:
          </p>
          <ul>
            <li><strong>L1 distance</strong> between predicted and expert latents.</li>
            <li><strong>Cosine similarity</strong> for directional alignment in latent space.</li>
          </ul>
          <p>
            The combined reward drives the VPM toward futures that are not only visually plausible
            but also dynamically consistent with expert rollouts.
          </p>

          <h3>2.2 GRPO-Based Optimization</h3>
          <p>
            We adopt <strong>Group Relative Policy Optimization (GRPO)</strong> to stabilize
            reinforcement learning over denoising trajectories:
          </p>
          <ul>
            <li>Form groups of candidate denoising trajectories for each scenario.</li>
            <li>Compute relative advantages and apply PPO-style clipping per group.</li>
            <li>Jointly update the VPM while leaving the downstream action model intact or lightly tuned.</li>
          </ul>
          <p>
            Compared to DDPO, GRPO yields more stable training and larger gains in long-horizon success rates.
          </p>
        </div>

        <div>
          <h3>2.3 Key Design Choices</h3>
          <div class="card">
            <div class="card-title">Hybrid Denoising</div>
            <div class="card-metric">1-step SDE vs. 5-step SDE</div>
            <p>
              We apply SDE-based stochasticity only at the <strong>first</strong> denoising step.
              This mitigates reward hacking that can arise when later steps are perturbed,
              while preserving enough exploration for effective optimization.
            </p>
          </div>

          <div class="card">
            <div class="card-title">Latent vs. Pixel Rewards</div>
            <p>
              Dyn-VPP computes rewards directly in <strong>latent space</strong> instead of decoded pixels.
              Latent rewards better capture task-relevant dynamics and semantics,
              leading to stronger performance than pixel-level objectives on CALVIN ABCâ†’D.
            </p>
          </div>

          <div class="card">
            <div class="card-title">Post-Training Schedule</div>
            <p>
              We first perform supervised fine-tuning of the VPM, then apply GRPO-based post-training
              for about <strong>1.4k steps</strong>, which empirically achieves the best trade-off between
              performance and stability.
            </p>
          </div>
        </div>
      </div>
    </section>

    <section id="results">
      <h2 class="section-title">3. Results Across Multiple Benchmarks</h2>
      <p class="section-subtitle">
        Dyn-VPP is evaluated on simulated long-horizon benchmarks and real robots, consistently
        improving over a strong VPM-based base policy across multiple task suites.
      </p>

      <div class="grid-2">
        <div>
          <h3>3.1 CALVIN ABCâ†’D</h3>
          <div class="tag-row">
            <span class="pill">Benchmark: CALVIN ABCâ†’D</span>
            <span class="pill">Long-horizon, 5-step</span>
            <span class="pill">Language-conditioned manipulation</span>
          </div>
          <p>
            Under the standard ABCâ†’D protocol, we compare Dyn-VPP with both VLM-based and
            VPM-based VLAs, including OpenVLA, Seer, VPP, Tri-VLA and others.
            Despite building directly on VPP, Dyn-VPPâ€™s post-training significantly improves
            task completion and average trajectory length.
          </p>
          <div class="card">
            <div class="card-title">Effectiveness of Better Visual Dynamics</div>
            <p>
              On CALVIN ABCâ†’D, our post-trained VPM + AGM achieves:
            </p>
            <ul>
              <li>Higher success for 1â€“5 tasks in a row vs. the base policy.</li>
              <li>Increased <strong>Avg. Len</strong>, <strong>Avg. ER</strong>,
                  and <strong>Avg. ERR</strong>, indicating richer visionâ€“action coupling.</li>
            </ul>
          </div>
          <div class="card">
            <div class="card-title">Comparison with SOTA VLAs</div>
            <p>
              Dyn-VPP outperforms strong baselines such as Tri-VLA and Seer on CALVIN ABCâ†’D,
              while requiring <strong>no additional network components or extra data</strong>
              beyond the base VPP checkpoint.
            </p>
          </div>
        </div>

        <div>
          <h3>3.2 L-CALVIN (10-step Long-Horizon)</h3>
          <div class="tag-row">
            <span class="pill">Benchmark: L-CALVIN</span>
            <span class="pill">10-step sequences</span>
            <span class="pill">ABCâ†’D generalization</span>
          </div>
          <p>
            L-CALVIN extends CALVIN to 10-step task sequences, providing a stricter test of
            long-horizon control. Dyn-VPP yields consistent improvements over the base policy
            for all horizons from 1 to 10 and boosts average length from 5.53 to 6.73.
          </p>

          <h3 style="margin-top:1.1rem;">3.3 Real-World Benchmarks</h3>
          <div class="tag-row">
            <span class="pill">Agibot Genie 01 (dual-arm)</span>
            <span class="pill">Multi-task real-world</span>
          </div>
          <p>
            We evaluate Dyn-VPP on multiple <strong>real-world benchmarks</strong>,
            including:
          </p>
          <ul>
            <li>Grasping a target object (apple) in clutter.</li>
            <li>Single-arm precise pick-and-place of a plate.</li>
            <li>Bimanual grasp-and-place of two bottles.</li>
          </ul>
          <p>
            Across all these tasks, Dyn-VPP consistently outperforms the base policy,
            demonstrating that improved visual dynamics translate into more reliable
            physical execution outside simulation.
          </p>
        </div>
      </div>

      <div style="margin-top: 1.5rem;">
        <h3>3.4 Visionâ€“Action Coupling Analysis</h3>
        <p>
          To understand <em>how</em> better visual dynamics help, we analyze the
          effective rank (ER) of the Jacobian \( \mathbf{J} = \partial \mathbf{a} / \partial \mathbf{v} \),
          and its normalized average ER ratio (ERR).
          Dyn-VPP systematically increases ER and ERR compared with the base policy,
          indicating that actions depend on a richer set of independent visual directions.
        </p>
      </div>
    </section>

    <section id="bibtex">
      <h2 class="section-title">4. BibTeX</h2>
      <p class="section-subtitle">
        If you find Dyn-VPP useful, please consider citing it:
      </p>
      <pre><code>@article{song2025dynvpp,
  title   = {Dyn-VPP: Video Prediction Policy Optimization for Improved Visual Dynamics},
  author  = {Wenxuan Song and Han Zhao and Fuhao Li and Ziyang Zhou and
             Pengxiang Ding and Donglin Wang and Haoang Li},
  journal = {arXiv preprint},
  year    = {2025}
}</code></pre>
    </section>

    <footer>
      <p>
        Dyn-VPP project page. Design inspired by
        <a href="https://mimic-video.github.io/" target="_blank" rel="noreferrer">mimic-video</a>
        and
        <a href="https://vla-adapter.github.io/" target="_blank" rel="noreferrer">VLA-Adapter</a>.
      </p>
    </footer>
  </main>
</body>
</html>